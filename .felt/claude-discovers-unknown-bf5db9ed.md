---
title: Claude discovers unknown frameworks — show as emergence and danger
status: closed
kind: spec
priority: 2
created-at: 2026-01-28T12:53:28.647076+01:00
closed-at: 2026-01-28T12:53:28.647083+01:00
close-reason: |-
    Anecdote from co-runner: Building a UI, Claude discovered frameworks they didn't know about and made good choices about which to use — choices the user 'could not have prompted it to do.'

    Teaching moment: Even without mentioning spins in the prompt, Claude may structure the implementation using spin formalism because it's in TreeCorr's codebase.

    Show this as: 'See, you described a high-level thing, it's actually going deeper than your own understanding.'

    Which is also a natural segue into: the dangers of AI doing things beyond your comprehension. How do you verify what you don't understand?
---
