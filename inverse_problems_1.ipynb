{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Inverse Problems\n",
    "\n",
    "---\n",
    "\n",
    "> Author: <font color='#f78c40'>Samuel Farrens</font>    \n",
    "> Year: 2018 (updated in 2023)  \n",
    "> Email: [samuel.farrens@cea.fr](mailto:samuel.farrens@cea.fr)  \n",
    "> Website: <a href=\"https://sfarrens.github.io\" target=\"_blank\">https://sfarrens.github.io</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contents\n",
    " \n",
    "1. [Set-Up](#Set-Up)\n",
    "1. [Introduction](#Introduction)\n",
    " * [Objective](#Objective)\n",
    " * [Inverse Problem Definition](#Inverse-Problem-Definition)\n",
    "1. [Linear Regression](#Linear-Regression)\n",
    " * [Mathematical Representation](#Mathematical-Representation)\n",
    " * [Straight Line Example](#Straight-Line-Example)\n",
    "1. [Linear Regression Exercise](#Linear-Regression-Exercise)\n",
    " * [Equation of a Polynomial Line](#Equation-of-a-Polynomial-Line)\n",
    "1. [Gradient Descent](#Gradient-Descent)\n",
    " * [L2 Norm](#L2-Norm)\n",
    " * [Linear Regression Application](#Linear-Regression-Application)\n",
    "1. [Gradient Descent Exercise](#Gradient-Descent-Exercise)\n",
    "1. [Hints](#Hints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Set-Up\n",
    "\n",
    "Here we will import a couple of packages that will be needed throughout the notebook. \n",
    "\n",
    "Users new to Jupyter notebooks should note that cells are executed by pressing <kbd>SHIFT</kbd>+<kbd>ENTER</kbd> (&#x21E7;+ &#x23ce;). See <a href=\"https://jupyter-notebook.readthedocs.io/en/stable/\" target_=\"blanck\">here</a> for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import the numpy package with the alias np.\n",
    "import numpy as np           \n",
    "\n",
    "# Import interactive widgets.\n",
    "from ipywidgets.widgets import interact, IntSlider, FloatSlider\n",
    "\n",
    "# Import tutorial plotting functions.\n",
    "from sparsity_tutorial import plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that the plotting routines have been abstracted to keep the code cells as simple as possible. You can run\n",
    "\n",
    "```python\n",
    "help(plot)\n",
    "```\n",
    "\n",
    "to see the expected inputs for these routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### <font color='#007acc'>Objective</font>\n",
    "\n",
    "The objective of this notebook is to introduce the concept of an inverse problem. The focus will be on linear inverse problems, which are those that can be modelled using linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <font color='#007acc'>Inverse Problem Definition</font>\n",
    "\n",
    "A standard \"forward\" problem is one in which data are obtained from model parameters, *i.e.*\n",
    "\n",
    "$$\\text{Model}\\rightarrow\\text{Data,}$$\n",
    "\n",
    "while an inverse problem is one in which the model parameters are determined from the data, *i.e.*\n",
    "\n",
    "$$\\text{Data}\\rightarrow\\text{Model.}$$\n",
    "\n",
    "In other words, with an inverse problem one attempts to obtain information about a physical system from observed measurements. This can be very useful as some model parameters can not be measured directly.\n",
    "\n",
    "Inverse problems have been applied to various topics such as oceanography, weather prediction, astrophysics, medical imaging and geophysics.\n",
    "\n",
    "See the following links for more information about inverse problems.\n",
    "* [Wikipedia](https://en.wikipedia.org/wiki/Inverse_problem)\n",
    "* [Scatting Ideas](https://cmontalto.wordpress.com/2013/03/08/what-are-inverse-problems/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "One of the simplest and most intuitive applications of an inverse problem is fitting a line to a set of data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <font color='#007acc'>Mathematical Representation</font>\n",
    "\n",
    "We will start by defining some variables. We will use $y \\in \\mathbb{R}^{m}$ to represent data measurements/observations, $a \\in \\mathbb{R}^{n}$ to represent a given model, and $H \\in \\mathbb{R}^{m \\times n}$ to represent a matrix of equations that relate the model parameters to the measured data. Now we can pose a linear inverse problem of the following form\n",
    "\n",
    "$$y = Ha$$\n",
    "\n",
    "When dealing with forward problems $a$ and $H$ are known and can be used to obtain $y$. For inverse problems $y$ and $H$ are known and can be used to obtain $a$.\n",
    "\n",
    "> Note: Here we use $a$ for our model vector to avoid confusion with data points on the $x$-axis. Commonly (and in the other tutorials) the model vector will be represented by the variable $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <font color='#007acc'>Straight Line Example</font>  \n",
    "\n",
    "The best place to start is with something that should be intimately familiar to anyone with even the most basic background in mathematics, a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Equation of a Straight Line\n",
    "\n",
    "A simple straight line can be represented with the following well known equation\n",
    "\n",
    "$$y = mx + b$$\n",
    "\n",
    "where $m$ is the gradient or slope of the line and $b$ is the point where the line intercepts the $y$-axis. In Python we can implement the following function to represent this equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This defines a function called y_func with input variables x, m and b, and returns the values of mx + b.\n",
    "def y_func(x, m, b):\n",
    "    \n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Forward Problem\n",
    "\n",
    "A straightforward way to approach this problem is, when we have some data on the x-axis\n",
    "\n",
    "$$x = \\begin{bmatrix} 8 & 2 & 11 & 6 & 5 & 4 & 12 & 9 & 6 & 11 \\end{bmatrix}$$\n",
    "\n",
    "and we know the slope and intercept of the line,\n",
    "\n",
    "$$m = -1.1$$\n",
    "\n",
    "$$b = 14,$$\n",
    "\n",
    "to simply plug this information into the equation above to get the corresponding data on the y-axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To think of the of this in terms of a linear forward problem, however, we need to know that the slope and intercept correspond to our model parameters and the data on the x-axis need to be converted into a matrix operator. So, a better way of representing our line equation would be:\n",
    "\n",
    "$$y = a_0x^0 + a_1x^1$$\n",
    "\n",
    "Then, in terms of our forward problem formulation ($y=Ha$), \n",
    "\n",
    "$$a = \\begin{bmatrix} a_0 & a_1 \\end{bmatrix} = \\begin{bmatrix} 14 & -1.1 \\end{bmatrix}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$H = \\begin{bmatrix} x^0 & x^1 \\end{bmatrix} = \\begin{bmatrix} \n",
    "1 & 8 \\\\\n",
    "1 & 2 \\\\\n",
    "1 & 11 \\\\\n",
    "1 & 6 \\\\\n",
    "1 & 5 \\\\\n",
    "1 & 4 \\\\ \n",
    "1 & 12 \\\\\n",
    "1 & 9 \\\\\n",
    "1 & 6 \\\\\n",
    "1 & 1 \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can implement this in Python as follows. First we define $a$ and $H$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set points in x-axis.\n",
    "x = np.array([8, 2, 11, 6, 5, 4, 12, 9, 6, 1])\n",
    "\n",
    "# Define the H matrix operator.\n",
    "H = np.array([np.ones(x.size), x]).T\n",
    "\n",
    "# Set the model values.\n",
    "a = np.array([14.0, -1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "then we can use these values to determine $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> Note that the `@` operator in the following cell is equivalent to [`np.matmul`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) or [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Y.\n",
    "y = H @ a  \n",
    "\n",
    "# Plot results.            \n",
    "plot.regression_plot(data=(x, y), model=(np.arange(14), y_func(np.arange(14), a[1], a[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As our data has been produced using the model, the fit is perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inverse Problem\n",
    "\n",
    "For the inverse problem we will assume we already know $y$ and use this to work out $a$. So, in other words, we already have a set of data points, $(x, y)$, and we want to work out the model parameters, $a$, of the best fitting line.\n",
    "\n",
    "$$x = \\begin{bmatrix} 8 & 2 & 11 & 6 & 5 & 4 & 12 & 9 & 6 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$y = \\begin{bmatrix} 3 & 10 & 3 & 6 & 8 & 12 & 1 & 4 & 9 & 14 \\end{bmatrix}$$\n",
    "\n",
    "The matrix $H$ remains unchanged\n",
    "\n",
    "$$H = \\begin{bmatrix} \n",
    "1 & 8 \\\\\n",
    "1 & 2 \\\\\n",
    "1 & 11 \\\\\n",
    "1 & 6 \\\\\n",
    "1 & 5 \\\\\n",
    "1 & 4 \\\\ \n",
    "1 & 12 \\\\\n",
    "1 & 9 \\\\\n",
    "1 & 6 \\\\\n",
    "1 & 1 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "but, to solve this problem, we need to invert $H$ *i.e.*\n",
    "\n",
    "$$a = H^{-1}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "however, $H$ is not a square matrix, which means it cannot be inverted directly. So, first we need to multiply by $H^{T}$.\n",
    "\n",
    "> Because $H \\in \\mathbb{R}^{m \\times n}$ and $H^{T} \\in \\mathbb{R}^{n \\times m}$ so $H^{T}H \\in \\mathbb{R}^{n \\times n}$\n",
    "\n",
    "So, the solution is\n",
    "\n",
    "$$a = (H^TH)^{-1}H^{T}y$$\n",
    "\n",
    "which is often called the <a href=\"http://mathworld.wolfram.com/NormalEquation.html\" target_=\"blank\">Normal Equation</a>. We can implement this in python as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This function implements the normal equation\n",
    "def normal_eq(H, y):\n",
    "    \n",
    "    return np.linalg.inv(H.T @ H) @ H.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With this function we can attempt to fit a line to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Set points in y-axis.\n",
    "y = np.array([3, 10, 3, 6, 8, 12, 1, 4, 9, 14])\n",
    "\n",
    "# Find model parameters.\n",
    "a = normal_eq(H, y)\n",
    "\n",
    "print('a =', a)\n",
    "\n",
    "# Plot results.\n",
    "plot.regression_plot(data=(x, y), model=(np.arange(14), y_func(np.arange(14), a[1], a[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As you can see we can easily recover a good fit to the data points with only a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression Exercise\n",
    "\n",
    "For this exercise you should apply the techniques learned for fitting a straight line to a set of data points to a new set of data that requires a polynomial fit.\n",
    "\n",
    "### <font color='#007acc'>Equation of a Polynomial Line</font>  \n",
    "\n",
    "The expression for a straight line (*i.e.* $k = 1$) generalises to the following form for a $k^{\\textrm{th}}$ degree polynomial:\n",
    "\n",
    "$$y = a_0 + a_1x + a_2x^2 + ... + a_kx^k$$\n",
    "\n",
    "where $a_i$ are the polynomial coefficients. We can represent this in Python with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the new function y(x) for any kth degree polynomial.\n",
    "# This defines a function called y_func2 with input variables x and a, and returns the values \n",
    "# of a0 + a1x + a2x^2 + ...\n",
    "def y_func2(x, a):\n",
    "    \n",
    "    return sum([(a_i * x ** i) for i, a_i in enumerate(a)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The data for this exercise are the following:\n",
    "\n",
    "| x    | y     |\n",
    "|:----:|:-----:|\n",
    "| 0.00 | 0.486 | \n",
    "| 0.05 | 0.866 | \n",
    "| 0.10 | 0.944 | \n",
    "| 0.15 | 1.144 | \n",
    "| 0.20 | 1.103 | \n",
    "| 0.25 | 1.202 | \n",
    "| 0.30 | 1.166 |\n",
    "| 0.35 | 1.191 | \n",
    "| 0.40 | 1.124 | \n",
    "| 0.45 | 1.095 | \n",
    "| 0.50 | 1.122 |\n",
    "| 0.55 | 1.102 |\n",
    "| 0.60 | 1.099 |\n",
    "| 0.65 | 1.017 |\n",
    "| 0.70 | 1.111 |\n",
    "| 0.75 | 1.117 |\n",
    "| 0.80 | 1.152 |\n",
    "| 0.85 | 1.265 |\n",
    "| 0.90 | 1.380 |\n",
    "| 0.95 | 1.575 |\n",
    "| 1.00 | 1.857 |\n",
    "\n",
    "These values have already been defined for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The values for x and y.\n",
    "x_k = np.linspace(0.0, 1.0, 21)\n",
    "y_k = np.array([\n",
    "    0.486, 0.866, 0.944, 1.144, 1.103, 1.202, 1.166, \n",
    "    1.191, 1.124, 1.095, 1.122, 1.102, 1.099, 1.017, \n",
    "    1.111, 1.117, 1.152, 1.265, 1.380, 1.575, 1.857\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Your job is to define the matrix operator $H$ and find the model parameters $a$ by solving the inverse problem $y=Ha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <font color='#e42d2f'>EXERCISE</font>  \n",
    "\n",
    "Modify the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# YOU NEED TO EDIT THIS CELL #\n",
    "##############################\n",
    "\n",
    "# Define the matrix operator X here:\n",
    "H_k = None\n",
    "\n",
    "# Calculate the model parameters A here:\n",
    "a_k = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now you can test how well your model line fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Display the plot.\n",
    "if a_k is not None:\n",
    "    plot.regression_plot(data=(x_k, y_k), model=(x_k, y_func2(x_k, a_k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "In the previous examples we used an analytical approach to solve the inverse problem, however for many problems this is not possible (*e.g.* if the matrix $H$ is not invertable).\n",
    "\n",
    "An alternative is to try an iterative method such as <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target_=\"blank\">gradient descent</a>, which searches for the local optimum of a function from a given starting position.\n",
    "\n",
    "For this approach we want to define a convex function that measures the accuracy of a given reconstruction. One way to test the accuracy is to measure the residual, $y-Hx$, for a given estimate of $x$. With a convex function, like the l2-norm, we will be searching for the global minimum of the residual\n",
    "\n",
    "$$F(x) = \\frac{1}{2}\\|Hx-y\\|_2^2$$\n",
    "\n",
    "The corresponding gradient is\n",
    "\n",
    "$$\\nabla F(x) = H^{T}(Hx-y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <font color='#007acc'>L2 Norm</font>  \n",
    "\n",
    "The l2 or Euclidian norm is calculated as follows:\n",
    "\n",
    "$$\\|x\\|_2 = \\Big(\\sum_{i=1}^n|x_i|^2\\Big)^{\\frac{1}{2}}$$\n",
    "\n",
    "We can show that this function is convex as follows. First we define a range of $x$ values, the we calculate $\\|x\\|_2$ and $\\|x\\|_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set a range of x values\n",
    "x_range = np.around(np.linspace(-1, 1, 101), 2)\n",
    "# Calculate the l2-norm of x\n",
    "x_norm = np.array([np.linalg.norm(x_i) for x_i in x_range])\n",
    "# Calculate the squared l2-norm of x\n",
    "x_norm2 = x_norm ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's plot these functions.\n",
    "\n",
    "#### <font color='#fdbf00'>INTERACTIVE CELL</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# YOU SHOULD INTERACT WITH THIS CELL #\n",
    "######################################\n",
    "@interact(x_val=FloatSlider(value=-0.8, min=-1.0, max=1.0, step=0.2))\n",
    "def show_grad(x_val):\n",
    "        \n",
    "    # Calculate the gradient of the squared l2-norm \n",
    "    grad = 2 * x_val\n",
    "    # Calculate the tangent line at this point\n",
    "    tangent = x_val ** 2 + 2 * x_val * (x_range - x_val)\n",
    "    # Display\n",
    "    plot.grad_plot(x_range, x_norm, x_norm2, grad, tangent, index=int(np.where(x_range == x_val)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The plot shows that both $\\|x\\|_2$ and $\\|x\\|_2^2$ have a clearly defined global minimum. The plot also displays the gradient of $\\|x\\|_2^2$ at the point $x=-0.8$, try ajusting the position of this point and see what happens to the gradient.\n",
    "\n",
    "Unsurprisingly, as the point apporachs the minimum, the gradient tends to zero. Also, depending on which side of the minimum the point is, the gradient will be either positive or negative. Gradient descent captialises on this by iteratively updating solutions in the following way\n",
    "\n",
    "$$x_{n+1} = x_n - \\gamma \\nabla F(x_n)$$\n",
    "\n",
    "where $x_n$ corresponds to the solution at given iteration. So, as the gradient approaches zero and consequenly as $x$ approaches the global minimum, the solution will converge. \n",
    "\n",
    "Now, we can try this apporach to out linear regression problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <font color='#007acc'>Linear Regression Application</font> \n",
    "\n",
    "The cost function for our problem is\n",
    "\n",
    "$$F(a) = \\frac{1}{2}\\|Ha-y\\|_2^2$$\n",
    "\n",
    "where, as before, $a$ are the model parameters of the line we wish to fit. We can implement this as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Cost function for linear regression problem\n",
    "def cost_func(y, a, H):\n",
    "    \n",
    "    return 0.5 * np.linalg.norm(H @ a - y) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The gradient of this problem is given by\n",
    "\n",
    "$$\\nabla F(a) = H^{T}(Ha-y)$$\n",
    "\n",
    "which can be implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient of linear regression problem\n",
    "def grad(y, a, H):\n",
    "    \n",
    "    return H.T @ (H @ a - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The gradient descent algorithm is simply\n",
    "\n",
    "$$a_{n+1} = a_n - \\gamma \\nabla F(a_n)$$\n",
    "\n",
    "where $a_n$ is our current \"guess\" of the model parameters and $a_{n+1}$ are the updated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that performs gradient descent.\n",
    "def grad_desc(y, a_guess, H, grad, cost_func, gamma=None, n_iter=None):\n",
    "    \n",
    "    a_rec = a_guess\n",
    "    cost = []\n",
    "            \n",
    "    for _ in range(n_iter):\n",
    "        \n",
    "        a_rec = a_rec - gamma * grad(y, a_rec, H)\n",
    "        cost.append(cost_func(y, a_rec, H))\n",
    "        \n",
    "    return a_rec, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can test this approach on our first straight line example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <font color='#fdbf00'>INTERACTIVE CELL</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# YOU SHOULD INTERACT WITH THIS CELL #\n",
    "######################################\n",
    "\n",
    "@interact(n_iter=IntSlider(value=1, min=1, max=1001, step=100))\n",
    "def test_gradient_descent(n_iter):\n",
    "\n",
    "    a_new, cost = grad_desc(y, [0, 0], H, grad, cost_func, gamma=0.003, n_iter=n_iter)\n",
    "    \n",
    "    plot.regression_plot_cost(\n",
    "        data=(x, y), \n",
    "        model=(np.arange(14), y_func(np.arange(14), a_new[1], a_new[0])), \n",
    "        cost=cost,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can clearly see how the model parameters are improved after each iteration and as the cost function converges we are left with a good fit to the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent Exercise\n",
    "\n",
    "Apply the same gradient descent approach to the polynomial line data.\n",
    "\n",
    "**Questions and tasks**\n",
    "\n",
    "1. What is the cost function of this problem?\n",
    "1. What is the gradient of this problem?\n",
    "1. Plot your fit.\n",
    "1. How many iterations did it take to get a reasonable fit?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <font color='#e42d2f'>EXERCISE</font>\n",
    "\n",
    "Provide your solutions in the cell(s) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## <font color=\"#fdbf00\">Hints</a>\n",
    "\n",
    "**Linear Regression Exercise**\n",
    "\n",
    "1. You do not need to redefine the function for the normal equation. \n",
    "1. The polynomial should have as many degrees as there are good Indiana Jones films. \n",
    "\n",
    "**Gradient Descent Exercise**\n",
    "\n",
    "1. Make sure your first guess for $a$ has the right size.\n",
    "1. If your cost function is increasing you may need to decrease the value of $\\gamma$.\n",
    "1. If your cost function is decreasing too slowly you may need to increase the value of $\\gamma$.\n",
    "1. Put as many iterations as needed to converge. It may be a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Open next notebook ->](./sparsity_1.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
